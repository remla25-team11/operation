---
- name: Disable SSH host key checking on controller
  hosts: ctrl
  become: true
  tasks:
    - name: Ensure /etc/ansible directory exists
      ansible.builtin.file:
        path: /etc/ansible
        state: directory
        mode: '0755'

    - name: Create Ansible config to disable host key checking
      ansible.builtin.copy:
        dest: /etc/ansible/ansible.cfg
        content: |
          [defaults]
          host_key_checking = False

- name: Provision Kubernetes controller node
  hosts: ctrl
  become: true
  vars:
    kubernetes_version_short: "v1.30"
    kube_packages_version_suffix: "=1.30.0-1.1"
    kube_packages:
      - containerd
      - runc
      - "kubelet{{ kube_packages_version_suffix }}"
      - "kubeadm{{ kube_packages_version_suffix }}"
      - "kubectl{{ kube_packages_version_suffix }}"
    kubernetes_apt_key_url: "https://pkgs.k8s.io/core:/stable:/{{ kubernetes_version_short }}/deb/Release.key"
    kubernetes_apt_key_path: "/etc/apt/keyrings/kubernetes-apt-keyring.gpg"
    kubernetes_apt_repo_core_url: "https://pkgs.k8s.io/core:/stable:/{{ kubernetes_version_short }}/deb/"
    kubernetes_sources_list_file: "/etc/apt/sources.list.d/kubernetes.list"
    hosts_entries:
      - { ip: "192.168.56.100", name: "ctrl" }
      - { ip: "192.168.56.101", name: "node-1" }
      - { ip: "192.168.56.102", name: "node-2" }
    istio_version: "1.22.1"
    prometheus_stack_values_file_src: "files/prometheus-stack-values.yaml" # Relative to playbook
    prometheus_stack_values_file_dest: "/tmp/prometheus-stack-values.yaml" # On the VM

  tasks:
    - name: Disable swap
      ansible.builtin.shell: swapoff -a
      changed_when: false

    - name: Remove swap from fstab
      ansible.builtin.lineinfile:
        path: /etc/fstab
        regexp: '^\s*[^#]+\s+swap\s+'
        state: absent

    - name: Ensure k8s.conf for kernel modules exists and has content
      ansible.builtin.copy:
        dest: /etc/modules-load.d/k8s.conf
        content: |
          overlay
          br_netfilter
        mode: '0644'

    - name: Load required kernel modules (overlay)
      community.general.modprobe:
        name: overlay
        state: present
    - name: Load required kernel modules (br_netfilter)
      community.general.modprobe:
        name: br_netfilter
        state: present

    - name: Ensure /etc/sysctl.d/k8s.conf exists for k8s networking settings
      ansible.builtin.file:
        path: /etc/sysctl.d/k8s.conf
        state: touch
        mode: '0644'

    - name: Set sysctl values for k8s networking
      ansible.posix.sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        sysctl_file: /etc/sysctl.d/k8s.conf
        state: present
        reload: yes
      loop:
        - { name: net.ipv4.ip_forward, value: "1" }
        - { name: net.bridge.bridge-nf-call-iptables, value: "1" }
        - { name: net.bridge.bridge-nf-call-ip6tables, value: "1" }

    - name: Add hosts file entries
      ansible.builtin.lineinfile:
        path: /etc/hosts
        line: "{{ item.ip }} {{ item.name }}"
        state: present
      loop: "{{ hosts_entries }}"

    # --- APT Setup for Kubernetes ---
    - name: Update apt cache and install prerequisite packages (run once before K8s repo setup)
      ansible.builtin.apt:
        name:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
        state: present
        update_cache: yes

    - name: Ensure apt keyrings directory exists
      ansible.builtin.file:
        path: "/etc/apt/keyrings"
        state: directory
        mode: '0755'

    - name: Remove old/existing Kubernetes GPG key file (if any, for a clean state)
      ansible.builtin.file:
        path: "{{ kubernetes_apt_key_path }}"
        state: absent

    - name: Remove old/existing Kubernetes sources list file (if any, for a clean state)
      ansible.builtin.file:
        path: "{{ kubernetes_sources_list_file }}"
        state: absent

    - name: Download Kubernetes public signing key and store it
      ansible.builtin.shell:
        cmd: "curl -fsSL \"{{ kubernetes_apt_key_url }}\" | gpg --dearmor -o \"{{ kubernetes_apt_key_path }}\""
      changed_when: true

    - name: Ensure correct permissions for the Kubernetes GPG key
      ansible.builtin.file:
        path: "{{ kubernetes_apt_key_path }}"
        mode: '0644'
        state: file

    - name: Add Kubernetes apt repository
      ansible.builtin.apt_repository:
        repo: "deb [arch=amd64 signed-by={{ kubernetes_apt_key_path }}] {{ kubernetes_apt_repo_core_url }} /"
        state: present
        filename: "kubernetes"
        update_cache: yes
    # --- End APT Setup for Kubernetes ---

    - name: Install Kubernetes & Containerd packages
      ansible.builtin.apt:
        name: "{{ kube_packages }}"
        state: present
        allow_change_held_packages: yes

    - name: Hold Kubernetes package versions
      ansible.builtin.dpkg_selections:
        name: "{{ item | regex_replace('=.*$', '') }}"
        selection: hold
      loop: "{{ kube_packages | select('match', '^kube.*') | list }}"

    - name: Ensure containerd config directory exists
      ansible.builtin.file:
        path: /etc/containerd
        state: directory

    - name: Generate default containerd config and configure for Kubernetes
      ansible.builtin.shell: |
        containerd config default > /etc/containerd/config.toml
        sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
      args:
        creates: /etc/containerd/config.toml
      changed_when: true

    - name: Restart containerd service
      ansible.builtin.service:
        name: containerd
        state: restarted
        enabled: yes

    - name: Check if Kubernetes cluster is already initialized
      ansible.builtin.stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_config

    - name: Reset Kubernetes cluster if admin.conf does not exist (pre-initialization)
      ansible.builtin.shell: kubeadm reset -f
      args:
        warn: false
      changed_when: true
      failed_when: false
      when: not kubeadm_config.stat.exists

    - name: Initialize Kubernetes cluster with kubeadm
      ansible.builtin.shell: |
        kubeadm init \
          --apiserver-advertise-address=192.168.56.100 \
          --node-name ctrl \
          --pod-network-cidr=10.244.0.0/16
      when: not kubeadm_config.stat.exists
      register: kubeadm_init_result
      changed_when: "'Your Kubernetes control-plane has initialized successfully!' in kubeadm_init_result.stdout"

    - name: Create .kube directory for vagrant user
      ansible.builtin.file:
        path: /home/vagrant/.kube
        state: directory
        owner: vagrant
        group: vagrant
        mode: '0755'
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)


    - name: Copy admin.conf to vagrant user's .kube directory
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/vagrant/.kube/config
        remote_src: yes
        owner: vagrant
        group: vagrant
        mode: '0644'
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)

    - name: Allow scheduling pods on the control-plane node (ensure taint is removed)
      become_user: vagrant
      ansible.builtin.shell: "kubectl taint nodes ctrl node-role.kubernetes.io/control-plane- --overwrite --kubeconfig /home/vagrant/.kube/config"
      register: untaint_result
      failed_when: "untaint_result.rc != 0 and 'not found' not in untaint_result.stderr"
      changed_when: "'untainted' in untaint_result.stdout"
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)

    - name: Generate kubeadm join command for worker nodes
      ansible.builtin.shell: kubeadm token create --print-join-command
      register: join_command_output
      when: kubeadm_init_result.changed is defined and kubeadm_init_result.changed

    - name: Save join command to a temporary file on controller
      ansible.builtin.copy:
        content: "{{ join_command_output.stdout }}"
        dest: /tmp/kubeadm_join_command.txt
        mode: '0644'
      when: join_command_output.stdout is defined and join_command_output.stdout != ""

    - name: Copy Flannel CNI manifest to controller
      ansible.builtin.copy:
        src: files/kube-flannel.yml
        dest: /home/vagrant/kube-flannel.yml
        owner: vagrant
        group: vagrant
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)


    - name: Apply Flannel Pod network CNI
      become_user: vagrant
      ansible.builtin.shell: kubectl apply -f /home/vagrant/kube-flannel.yml --kubeconfig /home/vagrant/.kube/config
      register: flannel_apply_result
      changed_when: "'created' in flannel_apply_result.stdout or 'configured' in flannel_apply_result.stdout"
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)

    - name: Wait for at least one Flannel pod to be Running
      become_user: vagrant
      ansible.builtin.shell: "kubectl get pods -n kube-flannel -l app=flannel --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}' --kubeconfig /home/vagrant/.kube/config"
      register: flannel_pod_running
      until: flannel_pod_running.stdout != ""
      retries: 30
      delay: 10
      changed_when: false
      ignore_errors: yes
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)


    - name: Install Helm
      ansible.builtin.shell: |
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh
      args:
        creates: /usr/local/bin/helm
      changed_when: true

    # Kubernetes Dashboard
    - name: Add Kubernetes Dashboard Helm repo and update
      become_user: vagrant
      ansible.builtin.shell: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ && helm repo update
      changed_when: false

    - name: Install/Upgrade Kubernetes Dashboard via Helm
      become_user: vagrant
      ansible.builtin.shell: |
        helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
          --namespace kubernetes-dashboard --create-namespace \
          --kubeconfig /home/vagrant/.kube/config \
          --set fullnameOverride=kubernetes-dashboard \
          --set ingress.enabled=true \
          --set ingress.className=nginx \
          --set ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"=HTTPS \
          --set ingress.hosts[0].host=dashboard.local \
          --set ingress.hosts[0].paths[0].path=/ \
          --set ingress.hosts[0].paths[0].pathType=ImplementationSpecific \
          --set service.externalPort=443 \
          --set protocolHttp=true \
          --set tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set tolerations[0].operator=Exists \
          --set tolerations[0].effect=NoSchedule \
          --wait
      changed_when: true

    - name: Copy admin-user manifest for Dashboard access
      ansible.builtin.copy:
        src: files/admin-user.yaml
        dest: /home/vagrant/admin-user.yaml

    - name: Apply admin-user for Dashboard access
      become_user: vagrant
      ansible.builtin.shell: kubectl apply -f /home/vagrant/admin-user.yaml --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    # MetalLB
    - name: Create metallb-system namespace if not exists
      become_user: vagrant
      ansible.builtin.shell: kubectl create namespace metallb-system --dry-run=client -o yaml | kubectl apply -f - --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    - name: Install MetalLB using manifests from synced folder
      become_user: vagrant
      ansible.builtin.shell: kubectl apply -f /ansible/files/metallb-native.yaml --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    - name: Wait for MetalLB controller deployment to be ready
      become_user: vagrant
      ansible.builtin.shell: kubectl wait deployment controller -n metallb-system --for=condition=Available=True --timeout=120s --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    - name: Apply MetalLB IPAddressPool configuration
      become_user: vagrant
      ansible.builtin.shell: kubectl apply -f /ansible/files/ipaddresspool.yaml --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    - name: Apply MetalLB L2Advertisement configuration
      become_user: vagrant
      ansible.builtin.shell: kubectl apply -f /ansible/files/l2advertisement.yaml --kubeconfig /home/vagrant/.kube/config
      changed_when: false

    # NGINX Ingress Controller
    - name: Add ingress-nginx Helm repo and update
      become_user: vagrant
      ansible.builtin.shell: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx && helm repo update
      changed_when: false

    - name: Install/Upgrade NGINX Ingress Controller via Helm
      become_user: vagrant
      ansible.builtin.shell: |
        helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --kubeconfig /home/vagrant/.kube/config \
          --set controller.service.type=LoadBalancer \
          --set controller.ingressClassResource.name=nginx \
          --set controller.ingressClassResource.default=true \
          --set controller.ingressClassResource.controllerValue="k8s.io/ingress-nginx" \
          --set controller.admissionWebhooks.patch.nodeSelector."kubernetes\.io/os"=linux \
          --set controller.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set controller.tolerations[0].operator=Exists \
          --set controller.tolerations[0].effect=NoSchedule \
          --set controller.admissionWebhooks.patch.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set controller.admissionWebhooks.patch.tolerations[0].operator=Exists \
          --set controller.admissionWebhooks.patch.tolerations[0].effect=NoSchedule \
          --wait
      changed_when: true

    # Istio Service Mesh
    - name: Download Istio v{{ istio_version }}
      ansible.builtin.get_url:
        url: "https://github.com/istio/istio/releases/download/{{ istio_version }}/istio-{{ istio_version }}-linux-amd64.tar.gz"
        dest: "/tmp/istio-{{ istio_version }}.tar.gz"
        mode: '0644'

    - name: Ensure Istio installation directory exists (/home/vagrant/istio-{{ istio_version }})
      ansible.builtin.file:
        path: "/home/vagrant/istio-{{ istio_version }}"
        state: directory
        owner: vagrant
        group: vagrant

    - name: Unarchive Istio to /home/vagrant/ (will create istio-{{ istio_version }} subdir)
      ansible.builtin.unarchive:
        src: "/tmp/istio-{{ istio_version }}.tar.gz"
        dest: /home/vagrant/
        owner: vagrant
        group: vagrant
        remote_src: yes
        creates: "/home/vagrant/istio-{{ istio_version }}/bin/istioctl"

    - name: Check if Istio (istiod deployment) is already installed
      become_user: vagrant
      ansible.builtin.shell: "kubectl get deployment istiod -n istio-system --kubeconfig /home/vagrant/.kube/config"
      register: istio_check
      ignore_errors: true
      changed_when: false

    - name: Install Istio using 'demo' profile
      become_user: vagrant
      ansible.builtin.shell:
        cmd: "/home/vagrant/istio-{{ istio_version }}/bin/istioctl install --set profile=demo -y --kubeconfig /home/vagrant/.kube/config"
      when: istio_check.rc != 0
      changed_when: true

    - name: Wait for Istio control plane (istiod) deployment to be ready
      become_user: vagrant
      ansible.builtin.shell: >
        kubectl wait deployment istiod
        --for=condition=Available=True
        --namespace=istio-system
        --timeout=300s
        --kubeconfig /home/vagrant/.kube/config
      when: istio_check.rc != 0
      changed_when: false

    - name: Wait for Istio Ingress Gateway deployment to be ready
      become_user: vagrant
      ansible.builtin.shell: >
        kubectl wait deployment istio-ingressgateway
        --for=condition=Available=True
        --namespace=istio-system
        --timeout=300s
        --kubeconfig /home/vagrant/.kube/config
      when: istio_check.rc != 0
      changed_when: false

    # Kube-Prometheus-Stack (Prometheus & Grafana)
    - name: Add prometheus-community Helm repo and update
      become_user: vagrant
      ansible.builtin.shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts && helm repo update
      changed_when: false

    - name: Copy kube-prometheus-stack values file to VM
      ansible.builtin.copy:
        src: "{{ prometheus_stack_values_file_src }}" # From operation/ansible/files/
        dest: "{{ prometheus_stack_values_file_dest }}" # To /tmp/ on VM
        mode: '0644'

    - name: Install/Upgrade kube-prometheus-stack (Using values file)
      become_user: vagrant
      ansible.builtin.shell: |
        helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
          --namespace monitoring --create-namespace \
          --kubeconfig /home/vagrant/.kube/config \
          -f "{{ prometheus_stack_values_file_dest }}" \
          --wait --timeout=10m
      changed_when: true

    # Kiali
    - name: Add Kiali Helm repo and update
      become_user: vagrant
      ansible.builtin.shell: helm repo add kiali https://kiali.org/helm-charts && helm repo update
      changed_when: false

    - name: Install/Upgrade Kiali Server via Helm
      become_user: vagrant
      ansible.builtin.shell: |
        helm upgrade --install kiali-server kiali/kiali-server \
          --namespace istio-system \
          --kubeconfig /home/vagrant/.kube/config \
          --set auth.strategy=anonymous \
          --set external_services.prometheus.url="http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090" \
          --set external_services.grafana.url="http://prometheus-grafana.monitoring.svc.cluster.local" \
          --set external_services.tracing.enabled=true \
          --set external_services.tracing.provider="jaeger" \
          --set external_services.tracing.in_cluster_url="http://jaeger-query.istio-system.svc.cluster.local:16686" \
          --set ingress.enabled=true \
          --set ingress.class_name=nginx \
          --set ingress.override_yaml.hosts[0].host=kiali.local \
          --set ingress.override_yaml.hosts[0].paths[0].path=/ \
          --set ingress.override_yaml.hosts[0].paths[0].pathType=Prefix \
          --set deployment.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set deployment.tolerations[0].operator=Exists \
          --set deployment.tolerations[0].effect=NoSchedule \
          --wait
      changed_when: true

    # Copy kubeconfig to host via synced folder
    - name: Copy admin.conf to synced folder for host access
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /ansible/kubeconfig
        remote_src: yes
        owner: vagrant
        group: vagrant
        mode: '0644'
      when: kubeadm_config.stat.exists or (kubeadm_init_result.changed is defined and kubeadm_init_result.changed)