---
- name: Disable SSH host key checking
  hosts: ctrl
  become: true
  tasks:
    - name: Ensure /etc/ansible directory exists
      ansible.builtin.file:
        path: /etc/ansible
        state: directory
        mode: '0755' # Standard permissions for directories

    - name: Create Ansible config to disable host key checking
      copy:
        dest: /etc/ansible/ansible.cfg
        content: |
          [defaults]
          host_key_checking = False

- name: Provision Kubernetes controller node
  hosts: ctrl
  become: true
  vars:
    kube_packages:
      - containerd
      - runc
      - kubelet=1.32.4-1.1
      - kubeadm=1.32.4-1.1
      - kubectl=1.32.4-1.1
    kubernetes_apt_key_url: https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key
    kubernetes_apt_repo: "deb https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /"
    hosts_entries:
      - { ip: "192.168.56.100", name: "ctrl" }
      - { ip: "192.168.56.101", name: "node-1" }
      - { ip: "192.168.56.102", name: "node-2" }
    istio_version: "1.22.1"

  tasks:

    - name: Disable swap
      shell: swapoff -a

    - name: Remove swap from fstab
      lineinfile:
        path: /etc/fstab
        regexp: '^\s*[^#]+\s+swap\s+'
        state: absent

    - name: Load required kernel modules
      copy:
        dest: /etc/modules-load.d/k8s.conf
        content: |
          overlay
          br_netfilter

    - name: Load modules now
      modprobe:
        name: "{{ item }}"
      loop: [overlay, br_netfilter]

    - name: Set sysctl values
      sysctl:
        name: "{{ item.name }}"
        value: "{{ item.value }}"
        state: present
        reload: yes
      loop:
        - { name: net.ipv4.ip_forward, value: 1 }
        - { name: net.bridge.bridge-nf-call-iptables, value: 1 }
        - { name: net.bridge.bridge-nf-call-ip6tables, value: 1 }

    - name: Add hosts file entries
      lineinfile:
        path: /etc/hosts
        line: "{{ item.ip }} {{ item.name }}"
        state: present
      loop: "{{ hosts_entries }}"

    - name: Add Kubernetes apt key
      apt_key:
        url: "{{ kubernetes_apt_key_url }}"
        state: present

    - name: Add Kubernetes apt repo
      apt_repository:
        repo: "{{ kubernetes_apt_repo }}"
        state: present
        filename: kubernetes

    - name: Update apt cache
      apt:
        update_cache: yes

    - name: Install Ansible on controller
      apt:
        name: ansible
        state: present
        update_cache: yes

    - name: Install Kubernetes + containerd packages
      apt:
        name: "{{ kube_packages }}"
        state: present

    - name: Ensure containerd config directory exists
      file:
        path: /etc/containerd
        state: directory

    - name: Generate default containerd config
      shell: "containerd config default > /etc/containerd/config.toml"
      args:
        creates: /etc/containerd/config.toml

    - name: Configure containerd for Kubernetes
      lineinfile:
        path: /etc/containerd/config.toml
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
      loop:
        - { regexp: '^(\s*)SystemdCgroup\s*=.*$', line: '          SystemdCgroup = true' }
        - { regexp: '^(\s*)disable_apparmor\s*=.*$', line: '        disable_apparmor = true' }
        - { regexp: '^(\s*)sandbox_image\s*=.*$', line: '        sandbox_image = "registry.k8s.io/pause:3.10"' }

    - name: Restart containerd
      service:
        name: containerd
        state: restarted
        enabled: yes

    # ✅ Step 13: Initialize Kubernetes cluster
    - name: Reset Kubernetes if already initialized
      ansible.builtin.shell: kubeadm reset -f
      args:
        warn: false
      changed_when: true # Always report as changed to ensure subsequent tasks run
      failed_when: false # Do not fail if reset fails (e.g., if not initialized)
      become: true

    - name: Check if cluster is already initialized
      stat:
        path: /etc/kubernetes/admin.conf
      register: kubeadm_config

    - name: Initialize Kubernetes with kubeadm
      shell: |
        kubeadm init \
          --apiserver-advertise-address=192.168.56.100 \
          --node-name ctrl \
          --pod-network-cidr=10.244.0.0/16
      when: not kubeadm_config.stat.exists

    # ✅ Step 14: Set up kubectl for vagrant user
    - name: Create .kube directory for vagrant
      file:
        path: /home/vagrant/.kube
        state: directory
        owner: vagrant
        group: vagrant
        mode: 0755

    - name: Copy admin.conf to vagrant kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/vagrant/.kube/config
        remote_src: yes
        owner: vagrant
        group: vagrant
        mode: 0644

    - name: Remove control-plane taint from ctrl node to allow scheduling
      become_user: vagrant
      ansible.builtin.shell: "kubectl taint nodes ctrl node-role.kubernetes.io/control-plane- --overwrite"
      changed_when: false

    # Generate kubeadm join command and save to file
    - name: Generate kubeadm join command
      ansible.builtin.shell: kubeadm token create --print-join-command
      register: join_command_output
      become: true
      
    - name: Save join command to file for nodes to use
      copy:
        content: "{{ join_command_output.stdout }}"
        dest: /tmp/kubeadm_join_command.txt
        mode: '0644'


    # Step 22: Dashboard

    # installing Flannel
    - name: Copy Flannel CNI manifest to controller
      copy:
        src: kube-flannel.yml
        dest: /home/vagrant/kube-flannel.yml
        owner: vagrant
        group: vagrant
        mode: 0644

    - name: Apply Flannel Pod network
      become_user: vagrant
      shell: kubectl apply -f /home/vagrant/kube-flannel.yml --kubeconfig /home/vagrant/.kube/config

    - name: Wait for Flannel pods to be ready (robust check)
      become_user: vagrant
      shell: |
        set -e
        POD_NAME=""
        for i in $(seq 1 60); do # Try for 5 minutes (60 * 5s = 300s)
          POD_NAME=$(kubectl get pods -n kube-flannel -l app=flannel -o jsonpath='{.items[0].metadata.name}' --kubeconfig /home/vagrant/.kube/config 2>/dev/null || true)
          if [ -n "$POD_NAME" ]; then
            STATUS=$(kubectl get pod "$POD_NAME" -n kube-flannel -o jsonpath='{.status.phase}' --kubeconfig /home/vagrant/.kube/config)
            READY_COND=$(kubectl get pod "$POD_NAME" -n kube-flannel -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' --kubeconfig /home/vagrant/.kube/config)
            if [ "$STATUS" == "Running" ] && [ "$READY_COND" == "True" ]; then
              echo "Flannel pod $POD_NAME is Running and Ready."
              exit 0
            fi
          fi
          echo "Waiting for Flannel pod to be Running and Ready (attempt $i/60)..."
          sleep 5
        done
        echo "Flannel pod did not become Running and Ready within timeout."
        exit 1
      args:
        executable: /bin/bash

    # installing Helm
    - name: Add Helm GPG key
      apt_key:
        url: https://baltocdn.com/helm/signing.asc
        state: present

    - name: Add Helm apt repository
      apt_repository:
        repo: "deb https://baltocdn.com/helm/stable/debian/ all main"
        state: present
        filename: helm-stable

    - name: Install Helm
      apt:
        name: helm
        state: present
        update_cache: yes
    # ingress for dashboard

    - name: Create kubernetes-dashboard namespace
      become_user: vagrant
      shell: kubectl create namespace kubernetes-dashboard --kubeconfig /home/vagrant/.kube/config
      ignore_errors: true # Ignore if namespace already exists

    - name: Copy Dashboard Ingress manifest
      copy:
        src: dashboard-ingress.yaml
        dest: /home/vagrant/dashboard-ingress.yaml
        owner: vagrant
        group: vagrant
        mode: 0644

    - name: Apply Dashboard Ingress
      become_user: vagrant
      shell: kubectl apply -f /home/vagrant/dashboard-ingress.yaml --kubeconfig /home/vagrant/.kube/config



    - name: Add Kubernetes Dashboard Helm repo
      become_user: vagrant
      shell: helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
      args:
        creates: /home/vagrant/.dashboard_repo_added

    - name: Update Helm repositories
      become_user: vagrant
      shell: helm repo update

    - name: Install Kubernetes Dashboard via Helm
      become_user: vagrant
      shell: |
        helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
          --namespace kubernetes-dashboard --create-namespace \
          --kubeconfig /home/vagrant/.kube/config \
          --set fullnameOverride=kubernetes-dashboard \
          --set ingress.enabled=true \
          --set ingress.className=nginx \
          --set ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"=HTTPS \
          --set ingress.hosts[0].host=dashboard.local \
          --set ingress.hosts[0].paths[0].path=/ \
          --set ingress.hosts[0].paths[0].pathType=ImplementationSpecific \
          --set tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set tolerations[0].operator=Exists \
          --set tolerations[0].effect=NoSchedule

    - name: Copy admin-user manifest for Dashboard access
      ansible.builtin.copy:
        src: admin-user.yaml  # This file must exist in your ansible/ directory
        dest: /home/vagrant/admin-user.yaml
        owner: vagrant
        group: vagrant
        mode: '0644'

    - name: Apply admin-user.yaml to create service account and binding
      become_user: vagrant
      shell: kubectl apply -f /home/vagrant/admin-user.yaml --kubeconfig /home/vagrant/.kube/config

    #MetalLB
    - name: Ensure python3-venv is installed on ctrl
      ansible.builtin.apt:
        name: python3-venv
        state: present
        update_cache: yes
      become: true
      delegate_to: ctrl

    - name: Create virtual environment for Ansible k8s module on ctrl
      ansible.builtin.command: python3 -m venv /opt/venv/ansible
      args:
        creates: /opt/venv/ansible/bin/activate
      become: true
      delegate_to: ctrl

    - name: Install Python Kubernetes client and OpenShift client for Ansible k8s module
      ansible.builtin.pip:
        name:
          - kubernetes
          - openshift
        virtualenv: /opt/venv/ansible
        virtualenv_python: python3
      become: true # This needs sudo on the remote host
      delegate_to: ctrl # This needs to run on the ctrl VM
      
    - name: Set ansible_python_interpreter for ctrl to use the virtual environment
      ansible.builtin.set_fact:
        ansible_python_interpreter: /opt/venv/ansible/bin/python3
      delegate_to: ctrl # This fact needs to be set for the ctrl host

    - name: Create metallb-system namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: metallb-system
      become_user: vagrant
      args:
        kubeconfig: /home/vagrant/.kube/config

    - name: Generate MetalLB memberlist secret key
      ansible.builtin.shell: openssl rand -base64 128 | tr -d '\n'
      register: memberlist_secret_key
      run_once: true
      delegate_to: localhost
      become: false # Ensure this task runs without sudo on localhost

    - name: Create MetalLB memberlist secret
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: memberlist
            namespace: metallb-system
          stringData:
            secretkey: "{{ memberlist_secret_key.stdout }}"
      become_user: vagrant
      args:
        kubeconfig: /home/vagrant/.kube/config

    - name: Install MetalLB CRDs and components
      become_user: vagrant
      shell: kubectl apply -f /ansible/metallb-native.yaml --kubeconfig /home/vagrant/.kube/config
      #wait
    - name: Wait for MetalLB webhook/controller to be ready
      become_user: vagrant
      shell: |
        kubectl wait deployment controller \
          -n metallb-system \
          --for=condition=Available=True \
          --timeout=90s \
          --kubeconfig /home/vagrant/.kube/config

    - name: Apply MetalLB IP address pool
      become_user: vagrant
      shell: kubectl apply -f /ansible/ipaddresspool.yaml --kubeconfig /home/vagrant/.kube/config

    - name: Apply MetalLB L2 advertisement
      become_user: vagrant
      shell: kubectl apply -f /ansible/l2advertisement.yaml --kubeconfig /home/vagrant/.kube/config

    # ingress-nginx helm
    - name: Add ingress-nginx Helm repo
      become_user: vagrant
      shell: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

    - name: Update Helm repos (for Ingress)
      become_user: vagrant
      shell: helm repo update

    - name: Uninstall NGINX Ingress Controller if it exists in a pending state
      become_user: vagrant
      shell: helm uninstall ingress-nginx --namespace ingress-nginx
      args:
        warn: false
      changed_when: true
      failed_when: false # Ignore errors if release not found or already uninstalled

    # Add this task to your ctrl.yaml playbook
    - name: Copy admin.conf to synced folder for host access
      become: true
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /ansible/kubeconfig  # This will place it in the host's ansible/ folder
        remote_src: yes
        owner: vagrant # Set vagrant as owner for easy access from host
        group: vagrant
        mode: '0644'

    # ---------------------------------------------------------------------------
    # ISTIO SERVICE MESH INSTALLATION
    # ---------------------------------------------------------------------------
    - name: "Download Istio v{{ istio_version }}"
      ansible.builtin.get_url:
        url: "https://github.com/istio/istio/releases/download/{{ istio_version }}/istio-{{ istio_version }}-linux-amd64.tar.gz"
        dest: "/tmp/istio-{{ istio_version }}.tar.gz"
        mode: '0644'

    - name: "Unarchive Istio to vagrant home"
      ansible.builtin.unarchive:
        src: "/tmp/istio-{{ istio_version }}.tar.gz"
        dest: /home/vagrant/
        owner: vagrant
        group: vagrant
        remote_src: yes # This is crucial, as the archive is on the remote machine

    - name: "Check if Istio is already installed (idempotency)"
      become_user: vagrant
      ansible.builtin.shell: "kubectl get deployment istiod -n istio-system --kubeconfig /home/vagrant/.kube/config"
      register: istio_check
      ignore_errors: true
      changed_when: false

    - name: "Install Istio using the 'minimal' profile"
      become_user: vagrant
      ansible.builtin.shell:
        cmd: "./istio-{{ istio_version }}/bin/istioctl install --set profile=minimal -y"
        chdir: /home/vagrant # Run the command from the vagrant home directory
      when: istio_check.rc != 0 # Only run this if the 'istiod' deployment was not found

    - name: "Wait for Istio control plane (istiod) to be ready"
      become_user: vagrant
      ansible.builtin.shell: >
        kubectl wait deployment istiod
        --for=condition=Available=True
        --namespace=istio-system
        --timeout=300s
        --kubeconfig /home/vagrant/.kube/config
      when: istio_check.rc != 0 # Only wait if we just performed the installation

    - name: Install NGINX Ingress Controller
      become_user: vagrant
      shell: |
        helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
          --namespace ingress-nginx --create-namespace \
          --kubeconfig /home/vagrant/.kube/config \
          --set controller.service.type=LoadBalancer \
          --set controller.ingressClass=nginx \
          --set controller.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set controller.tolerations[0].operator=Exists \
          --set controller.tolerations[0].effect=NoSchedule \
          --set controller.admissionWebhooks.patch.nodeSelector."kubernetes\.io/os"=linux \
          --set controller.admissionWebhooks.patch.tolerations[0].key=node-role.kubernetes.io/control-plane \
          --set controller.admissionWebhooks.patch.tolerations[0].operator=Exists \
          --set controller.admissionWebhooks.patch.tolerations[0].effect=NoSchedule
